{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_gan as tfgan\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append( os.path.abspath('..') )\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('CIFAR').mkdir(exist_ok=True)\n",
    "os.chdir('CIFAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()\n",
    "data = np.concatenate((x_train, x_test))\n",
    "data = (data.astype('float32') - 127.5) / 127.5  # normalize to [-1, 1]\n",
    "assert data.shape == (60000, 32, 32, 3)  # (batch, height, width, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model_transp_conv(latent_dims):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(4*4*256, input_shape=(latent_dims,)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        #  tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Reshape((4, 4, 256)),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "\n",
    "        tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', activation='tanh')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model_upsample(latent_dims, interpolation):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(4*4*256, input_shape=(latent_dims,)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Reshape((4, 4, 256)),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=2, interpolation=interpolation),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=2, interpolation=interpolation),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=2, interpolation=interpolation),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', activation='tanh')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(32,32,3)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        # tf.keras.layers.GlobalAvgPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Wasserstein metric, the loss for the generator (G) given the critic (f) is given by:\n",
    "\n",
    "$$\n",
    "    -\\mathbb{E}_{z \\sim p(z)}\\bigl\\lbrack f\\bigl(G(z)\\bigr) \\bigr\\rbrack\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return -tf.reduce_mean(fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The critic loss tries to maximize:\n",
    "$$\n",
    "    \\max_{\\|f \\|_{L} \\leq 1} \\mathbb{E}_{x \\sim p_{data}} \\bigl\\lbrack f(x) \\bigr\\rbrack - \n",
    "    \\mathbb{E}_{z \\sim p_{z}} \\bigl\\lbrack f\\bigl(G(z)\\bigr) \\bigr\\rbrack\n",
    "$$\n",
    "\n",
    "Which is equivalent to minimizing the negative of this value, as shown in the function bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_loss(real_output, fake_output):\n",
    "    mu_real = tf.reduce_mean(real_output)\n",
    "    mu_fake = tf.reduce_mean(fake_output)\n",
    "    return mu_fake - mu_real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient penalty is given by:\n",
    "\n",
    "$$\n",
    "    \\mathbb{E}_{\\hat{x} \\sim p_{\\hat{x}}} \\bigl\\lbrack \n",
    "    \\bigl( \\|\\nabla_{\\hat{x}} f(\\hat{x})\\|_{2} - 1\\bigr)^2\n",
    "    \\bigr\\rbrack\n",
    "$$\n",
    "\n",
    "Where $\\hat{x}$ is a point between the real and fake data outputs from the critic, the point is randomly selected in a uniform distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real_samples, fake_samples):\n",
    "    differences = fake_samples - real_samples\n",
    "    alpha = tf.random.uniform(shape=(differences.shape[0], 1, 1, 1), minval=0.0, maxval=1.0)\n",
    "    interpolated = real_samples + alpha*differences\n",
    "    grads = tf.gradients(critic(interpolated), [interpolated])[0]\n",
    "    slopes = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1]))\n",
    "    return tf.reduce_mean((slopes-1.0)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_train_step(generator, critic, images, latent_dims, _lambda):\n",
    "    noise = tf.random.normal([images.shape[0], latent_dims])\n",
    "    with tf.GradientTape() as crit_tape:\n",
    "        generated_imgs = generator(noise, training=True)\n",
    "        real_output = critic(images, training=True)\n",
    "        fake_output = critic(generated_imgs, training=True)\n",
    "        penalty = gradient_penalty(critic, images, generated_imgs)\n",
    "        loss_C = critic_loss(real_output, fake_output) + _lambda * penalty\n",
    "    \n",
    "    grads_C = crit_tape.gradient(loss_C, critic.trainable_variables)\n",
    "    critic.optimizer.apply_gradients(zip(grads_C, critic.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(generator, critic, batch_size, latent_dims):\n",
    "    noise = tf.random.normal([batch_size, latent_dims])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_imgs = generator(noise, training=True)\n",
    "        fake_output = critic(generated_imgs, training=True)\n",
    "        loss_G = generator_loss(fake_output)\n",
    "    \n",
    "    grads_G = gen_tape.gradient(loss_G, generator.trainable_variables)\n",
    "    generator.optimizer.apply_gradients(zip(grads_G, generator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, critic, data, epochs, _lambda, n_critic=1, batch_size=32, callbacks=None):\n",
    "    latent_dims = generator.input_shape[1]\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(data.shape[0]).batch(batch_size)\n",
    "    iterator     = iter(dataset)\n",
    "    num_batches  = 1 + (data.shape[0] - 1) // batch_size\n",
    "    batches_left = True\n",
    "    batch_count  = 0\n",
    "    \n",
    "    \n",
    "    generator_step = tf.function(generator_train_step)\n",
    "    critic_step = tf.function(critic_train_step)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for c in callbacks:\n",
    "            c.on_epoch_begin(epoch=epoch + 1, generator=generator, discriminator=critic)\n",
    "        \n",
    "        batch_pbar = tqdm(total=num_batches, leave=False)\n",
    "        while batches_left:\n",
    "            for i in range(n_critic):\n",
    "                if batch_count == num_batches:\n",
    "                    batch_count = 0\n",
    "                    batches_left = False\n",
    "                    iterator = iter(dataset)\n",
    "                batch_count += 1\n",
    "                batch_pbar.update()\n",
    "                batch = iterator.get_next()\n",
    "                critic_step(generator, critic, batch, latent_dims, _lambda)\n",
    "            generator_step(generator, critic, batch_size, latent_dims)\n",
    "        batches_left = True\n",
    "        batch_pbar.update(num_batches)\n",
    "        batch_pbar.close()\n",
    "        \n",
    "        for c in callbacks:\n",
    "            c.on_epoch_end(epoch=epoch + 1, generator=generator, discriminator=critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Metrics classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the classifier that will be used to calculate the *Classifier Score* (CS) and *Fréchet Classifier Distance* (FCD). \\\n",
    "The features of the real data are also precalculated to avoid doing that for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.models.load_model('../../Classifiers/cifar.h5')\n",
    "feature_layer = classifier.get_layer('features')\n",
    "logits_layer = classifier.get_layer('logits')\n",
    "precalculated_features = utils.fn.calculate_features(classifier, feature_layer, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These were the hyperparameters tested for the final document. Training all of them simultaneously may take a long time, consider commenting out some options to run the tests individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIMS = 64\n",
    "BATCH_SIZE = 16\n",
    "LAMBDA = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_list = [\n",
    "    {'n_critic': 5, 'learning_rate': 2e-4, 'beta_1': 0.0, 'upsample': 'TrpConv' },\n",
    "    {'n_critic': 5, 'learning_rate': 2e-4, 'beta_1': 0.5, 'upsample': 'TrpConv' },\n",
    "    {'n_critic': 5, 'learning_rate': 2e-4, 'beta_1': 0.0, 'upsample': 'bilinear'},\n",
    "    {'n_critic': 5, 'learning_rate': 2e-4, 'beta_1': 0.5, 'upsample': 'bilinear'},\n",
    "    {'n_critic': 5, 'learning_rate': 2e-4, 'beta_1': 0.0, 'upsample': 'nearest' },\n",
    "    {'n_critic': 5, 'learning_rate': 2e-4, 'beta_1': 0.5, 'upsample': 'nearest' },\n",
    "    {'n_critic': 1, 'learning_rate': 1e-4, 'beta_1': 0.0, 'upsample': 'nearest' }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hparams in hparams_list:\n",
    "    dirname = 'NCRIT{}_beta1[{:.1f}]_LR{:.0e}_{}'.format(\n",
    "        hparams['n_critic'],\n",
    "        hparams['beta_1'],\n",
    "        hparams['learning_rate'],\n",
    "        hparams['upsample'].upper()\n",
    "    )\n",
    "    Path(dirname).mkdir(exist_ok=True)\n",
    "    \n",
    "    if hparams['upsample'] is 'TrpConv':\n",
    "        generator = generator_model_transp_conv(LATENT_DIMS)\n",
    "    else:\n",
    "        generator = generator_model_upsample(LATENT_DIMS, hparams['upsample'])\n",
    "    generator.optimizer = tf.keras.optimizers.Adam(hparams['learning_rate'], beta_1=hparams['beta_1'])\n",
    "    \n",
    "    critic = critic_model()\n",
    "    critic.optimizer = tf.keras.optimizers.Adam(hparams['learning_rate'], beta_1=hparams['beta_1'])\n",
    "    \n",
    "    ## Callbacks\n",
    "    timer = utils.callback.TimerCallback()\n",
    "    save_samples = utils.callback.SaveSamplesCallback(\n",
    "        path_format=os.path.join(dirname, 'epoch-{}'),\n",
    "        inputs=tf.random.normal((10*10, LATENT_DIMS)),\n",
    "        n_cols=10,\n",
    "        savefig_kwargs={'bbox_inches': 'tight', 'pad_inches': 0, 'dpi': 192},\n",
    "        transform_samples= lambda samples: (1 + samples) * 0.5,\n",
    "        grid_params={'border': 1, 'pad': 1, 'pad_value': 0}\n",
    "    )\n",
    "    metrics = utils.callback.MetricsCallback(\n",
    "        generator=generator,\n",
    "        classifier=classifier,\n",
    "        latent_dims=LATENT_DIMS,\n",
    "        feature_layer=feature_layer,\n",
    "        logits_layer=logits_layer,\n",
    "        n_samples=200*128,\n",
    "        batch_size=128,\n",
    "        precalculated_features=precalculated_features,\n",
    "        save_after=5, save_to=os.path.join(dirname, 'best.h5')\n",
    "    )\n",
    "    \n",
    "    ## Train and save results\n",
    "    train(\n",
    "        generator, critic, data, epochs=30,\n",
    "        _lambda=LAMBDA,\n",
    "        n_critic=hparams['n_critic'],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[timer, save_samples, metrics]\n",
    "    )\n",
    "    \n",
    "    metrics_obj = metrics.get_metrics()\n",
    "    metrics_obj['time'] = timer.get_time()\n",
    "    utils.fn.update_json_log(os.path.join(dirname, 'log.json'), metrics_obj)\n",
    "    \n",
    "    generator.save(os.path.join(dirname, 'generator.h5'), overwrite=True, save_format='h5')\n",
    "    critic.save   (os.path.join(dirname, 'critic.h5'   ), overwrite=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "In windows the command bellow is used to turn down the machine after the training finishes, very useful if you wanna let the computer running while you go to sleep :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !shutdown /s /t 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
