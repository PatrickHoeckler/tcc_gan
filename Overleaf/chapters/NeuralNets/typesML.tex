\section{Types of Machine Learning}
For a machine to learn it is necessary to have something for it to learn, some set of data that can point it to the desired behaviour. Depending on the type of data available it is possible to divide learning into 4 different categories: Supervised; Unsupervised; Semi-supervised; and Reinforcement Learning.

\subsection{Supervised Learning} \label{sub:supervised_learning}
The simplest way to teach a computer is to train it with both the input data and the expected output. For example, suppose a classifier that is trained on the \gls{MNIST} dataset, the goal of this classifier is to take a gray-scale image as input and return the corresponding digit as output. Since \gls{MNIST} also contains the labels, the training would consist of feeding the images to the learning algorithm and comparing the predicted digits with those given by the labels. The difference between the prediction and the real output can then be used to adjust the classifier in order to make future predictions more likely to be correct.

Learning problems where the data contains both the input and the desired output are known as Supervised Learning problems, this is currently the most common form of learning. The advantage of this approach is that it makes very explicit to the computer what is expected from it, resulting in generally easier learning when compared to other forms of learning that have incomplete data.

The biggest problem with supervised learning is producing the datasets in the first place, obtaining the input data is generally simple, however producing the corresponding outputs (e.g. labels for classification problems) can be very difficult. For problems like image colorization \cite{colorization2016} or super resolution \cite{ganSuperResolution2016} this is trivial (i.e. convert a colorized image to gray-scale, downscale high resolution images), however for a dataset like \gls{MNIST} it is necessary to have a human label every single one of the $70,000$ images. For bigger datasets like ImageNet \cite{imageNet2015} that contain millions of images, classified into thousands of different classes, and that have annotated bounding boxes for a significant number of the images, the process is very slow and expensive. Amazon Mechanical Turk 
\footnote{
    Mechanical Turk page: \url{https://www.mturk.com}
}%
is often used for such tasks.

\subsection{Unsupervised Learning} \label{sub:unsupervised_learning}
In contrast with supervised learning where all the data is often costly labelled, unsupervised learning refers to problems where only the input data is available and the network has no explicit notion of the desired outputs. These types of problems have a huge potential since the amount of unlabeled data is much larger, however, learning without labels is also much more difficult and it is still an active area of research.

Situations where unsupervised learning can be used are more rare and may require some specific prior knowledge of the datasets to work properly. The \gls{MNIST} dataset can be used again as a simple example to understand how this type of learning works. Suppose that it is desired to build a classifier for handwritten digits, but the only dataset available is \gls{MNIST} stripped of its labels; it may seem impossible to learn anything from this since all the machine sees are just images without any notion of right or wrong classification.

The trick is to exploit the prior knowledge of the problem and the distributions of the dataset in order to make progress. Since the objective is to build a digit classifier, then one prior information is that the number of possible classes will be 10, or 11 if a class ``Not a digit'' is also included.

Also consider the size of the input space, for \gls{MNIST} this is a $28{\times}28 = 784$ dimensional space, and with gray-scale images the number of different possible inputs is $256^{28\cdot28} = 2^{6,272} \approx 10^{1888}$. In such high dimensional spaces, all of the sensible inputs (e.g. images of digits) are just a tiny fraction of the whole input volume. This holds true for basically all situations, any random sample from an input space (pixel values, letters, audio amplitude) will almost always fail to produce the structured data present in the real world (pictures, words and phrases, human voice) \cite[chap. 5]{deepLearningBook2016} \textbf{--} it can be said that real life data is \textit{sparse} on the input space.

Along with sparsity, real data is also not evenly spaced on these high dimensional spaces, but it is instead concentrated around a relatively small number of clusters. \textcite[chap. 5]{deepLearningBook2016} argue this point by noting that it is possible to take similar images and apply some transformations, like moving objects or changing the light, in order to move from one image to another in the cluster. For example, one could take an image of the number 0 in \gls{MNIST}, add some slight rotations or change the width of the strokes, and end up with a different image that still represented the number 0. However a transformation from the number 0 to the number 3 does not seem to be so simple, so at least in an intuitive sense this example gives an idea of why the sparse data is also clustered around some points.

Using this knowledge of clusters and the number of classes expected in a problem like \gls{MNIST}, one can build an algorithm like \gls{KNN} to divide the input space into a desired number of regions, using a measure of similarity between inputs from the dataset as a guide for the divisions. Without using any labels the resulting algorithm is able to tell in which of the regions a given input belongs. For the \gls{MNIST} case, these regions have strong correlation with the corresponding digit \textbf{--} \cite{fashionMNIST2017} obtained accuracies of above 95\% with this technique.

Of course this approach is very dependent of the dataset and type of problem, it is not always that unsupervised learning will be so straight forward or that the divided regions will correlate well with the desired output.

\acp{GAN}, the main focus of this document, are also primarily unsupervised learning approaches. In \autoref{cha:gans} it will be discussed in detail how they work and learn from the underlying data distribution.

\subsection{Semi-supervised and Reinforcement Learning} \label{sub:semi-supervised_and_RL}
Semi-supervised learning is an intermediate between fully supervised and unsupervised learning, it concerns situations where the dataset is only partially labeled, the idea behind it is to use the vast amounts of unlabelled data to support the training from the relatively small number of labeled data. Since unlabeled data is much more easily available, semi-supervised learning has a lot of potential for building better models by fully leveraging the available data \cite{semi_supervised2005}.

Reinforcement learning is particularly different from the other types of learning, its objective is to teach an agent to interact with a changing environment; learning does not occur with a dataset, but is instead achieved through trial-and-error where the agent is rewarded or punished depending on its actions \cite{reinforcement_learning1996}. AlphaZero is an example of a reinforcement learning agent, it learned to play chess, shogi, and Go entirely through self-play \cite{alphaZero2017}.
