\chapter{Conclusion} \label{cha:conclusion}
After all the experiments the one thing that can be confirmed is that \acp{GAN} have lived up to their infamy of being hard to train. But with the theoretical basis and experimental evidence arranged in this document it is possible to make some guidelines of what to consider when building a \gls{GAN} for generating images.

First, and most important, if the dataset contains labels they should be used in the model. This not only allows for more control when using the generator later, but it also significantly increases the performance more than any other technique tested.

About the architecture to use, the standard \gls{GAN} should be avoided, there is no reason to prefer this method over any other. Particularly, the \gls{DCGAN} is a good starting point, it is relatively simple to create and it produced good results in all tests, although the selection of hyperparameters may be the most difficult part of the process. But the most promising type is the \gls{WGAN-GP}, it has the desirable properties of the original \gls{WGAN} without having the downside of clipping parameters. The \gls{WGAN-GP} produced better or similar results to the \gls{DCGAN} in all experiments, it also seemed to be more resistant to hyperparameter changes, making the process of search easier, and lastly it produced visually more pleasing results in the more complex CelebA dataset.

If training a \gls{WGAN-GP}, the best number for iterations in the discriminator seems to be 1, however the tests cannot say this with a high degree of confidence, it may be that this accelerates training in the beginning but makes convergence harder later in training. However, the results suggest that at the start of training low values of iterations work better.

When upsampling the latent vector, a good technique to choose is nearest neighbour upsampling followed by a normal dimension preserving convolution, this avoids the problem with artifacts produced by the transposed convolution, and it also showed similar results in the experiments, although lagging somewhat behind in some cases. Bilinear convolutions have shown to produce odd smoothing of the images, initially this method should be avoided, unless the dataset also shares this property.

The Adam optimizer should be the default option, maybe replaced by RMSProp when the momentum is not desired, but the $\beta_1$ hyperparameter could be set to 0 for this as well. And this term is something to look for in the case of divergence, a default value of $0.9$ alone can break convergence.

Batch normalization is also something that can stop the model from converging, the experiments made were inconclusive of when it does or does not work. It can speed up training, but it is a likely candidate to look for if the model is not producing good results.

Label smoothing was also left inconclusive, although some tests showed some slight advantages of using it, it was not significative to be able to confirm their effectiveness. However, they never showed any sign of reducing the performance of the models, so it is something worth considering to add in order to check if it benefits a determined situation. One disadvantage of this method is that it only works for the \gls{DCGAN} case, so it may not be worth giving up the benefits of the \gls{WGAN-GP} for it.

Other than that, the number of dimensions does not need to be very high, only enough as to not reduce the capacity of the generator. The highest value used in the experiments was $256$ for the CelebA model, and this is probably enough for the majority of cases. Small batch sizes seems like a good choice, in the order of $16$ to $32$ produced generally good results in the experiments, if the model is not working correctly the batches should probably not be the main culprit.

These are also not an exhaustive list of all that was proposed as improvements for \acp{GAN}, many models build on some of the ideas explored in this document. To build better models it is important to understand the concepts and know how to expand them by incorporating new techniques. Rather than the end, the information contained here should be used as a starting point before diving deeper into more advanced territory.
