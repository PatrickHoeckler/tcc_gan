{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "425f9469",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d546a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_gan as tfgan\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append( os.path.abspath('..') )\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e6437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('CIFAR').mkdir(exist_ok=True)\n",
    "os.chdir('CIFAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c44f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = tf.keras.datasets.cifar10.load_data()\n",
    "data = np.concatenate((x_train, x_test))\n",
    "data = (data.astype('float32') - 127.5) / 127.5  # normalize to [-1, 1]\n",
    "assert data.shape == (60000, 32, 32, 3)  # (batch, height, width, channel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b1ad5d",
   "metadata": {},
   "source": [
    "## 1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d29e0b7",
   "metadata": {},
   "source": [
    "### 1.1 Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858afeed",
   "metadata": {},
   "source": [
    "CIFAR-10 was very hard to get to work, for these tests it was found that not using batch normalization or dropout and changing some hyperparameters made things work.\n",
    "\n",
    "However, in hindsight the thing that was probably causing problems was the use of the momentum term $\\beta_1$ in the Adam optimizer. Maybe Batch Normalization and Dropout would also work now, but this was the way that it was done for the document, so they will be commented out here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d7d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model_transp_conv(latent_dims):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(4*4*256, input_shape=(latent_dims,)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Reshape((4, 4, 256)),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "\n",
    "        tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', activation='tanh')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc33b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model_bilinear(latent_dims):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(4*4*256, input_shape=(latent_dims,)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Reshape((4, 4, 256)),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear'),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear'),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        \n",
    "        tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear'),\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(3, kernel_size=3, strides=1, padding='same', activation='tanh')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faca913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', input_shape=(32,32,3)),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "        \n",
    "        tf.keras.layers.Conv2D(filters=256, kernel_size=3, strides=1, padding='same'),\n",
    "        tf.keras.layers.LeakyReLU(0.2),\n",
    "        # tf.keras.layers.BatchNormalization(),\n",
    "        # tf.keras.layers.Dropout(0.3),\n",
    "\n",
    "        # tf.keras.layers.GlobalAvgPool2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23770fe5",
   "metadata": {},
   "source": [
    "### 1.2 Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f8296",
   "metadata": {},
   "source": [
    "The binary cross entropy (BCE) between $y$ and $\\hat{y}$ is calculated as:\n",
    "\n",
    "$$\n",
    "    \\mathrm{BCE}(y, \\hat{y}) = - y \\log\\left(\\hat{y}\\right) - (1-y) \\log\\left(1 - \\hat{y}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb4085",
   "metadata": {},
   "source": [
    "The generator tries to maximize the chance of the discriminator being wrong. This is equivalent of trying to minimize the following loss function:\n",
    "\n",
    "$$\n",
    "    J^{(G)} = -\\log\\bigl(D\\bigl(G(z)\\bigr)\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7e009",
   "metadata": {},
   "source": [
    "The discriminator tries to correctly classify real data as real and fake data as fake. This is equivalent to minimizing the following loss function:\n",
    "\n",
    "$$\n",
    "    J^{(D)} = -\\log\\bigr(D(x)\\bigl) - \\log\\bigl(1 - D\\bigl(G(z)\\bigr)\\bigr)\n",
    "$$\n",
    "\n",
    "Here we scale down the loss by a factor of $\\;0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb44540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_normal(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return 0.5 * (real_loss + fake_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f873b",
   "metadata": {},
   "source": [
    "This function applies one sided label smoothing of $\\:0.9\\:$ to the discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_smooth(real_output, fake_output):\n",
    "    real_loss = cross_entropy(0.9 * tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return 0.5 * (real_loss + fake_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da20c419",
   "metadata": {},
   "source": [
    "## 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c70999",
   "metadata": {},
   "source": [
    "### 2.1 Main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(generator, discriminator, images, latent_dims):\n",
    "    noise = tf.random.normal([images.shape[0], latent_dims])\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        generated_imgs = generator(noise, training=True)\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_imgs, training=True)\n",
    "        loss_D = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    grads_D = disc_tape.gradient(loss_D, discriminator.trainable_variables)\n",
    "    discriminator.optimizer.apply_gradients(zip(grads_D, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d415184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(generator, discriminator, batch_size, latent_dims):\n",
    "    noise = tf.random.normal([batch_size, latent_dims])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_imgs = generator(noise, training=True)\n",
    "        fake_output = discriminator(generated_imgs, training=True)\n",
    "        loss_G = generator_loss(fake_output)\n",
    "    \n",
    "    grads_G = gen_tape.gradient(loss_G, generator.trainable_variables)\n",
    "    generator.optimizer.apply_gradients(zip(grads_G, generator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f95fa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, data, epochs, batch_size=None, callbacks=None):\n",
    "    latent_dims = generator.input_shape[1]\n",
    "    batch_size = batch_size if batch_size is not None else 32\n",
    "    num_batches = 1 + (data.shape[0] - 1) // batch_size\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(data).shuffle(data.shape[0]).batch(batch_size)\n",
    "    \n",
    "    generator_step = tf.function(generator_train_step)\n",
    "    discriminator_step = tf.function(discriminator_train_step)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for c in callbacks:\n",
    "            c.on_epoch_begin(epoch=epoch + 1, generator=generator, discriminator=discriminator)\n",
    "            \n",
    "        for batch in tqdm(dataset, leave=False, total=num_batches):\n",
    "            discriminator_step(generator, discriminator, batch, latent_dims)\n",
    "            generator_step(generator, discriminator, batch_size, latent_dims)\n",
    "        \n",
    "        for c in callbacks:\n",
    "            c.on_epoch_end(epoch=epoch + 1, generator=generator, discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2448bd9",
   "metadata": {},
   "source": [
    "### 2.2 Metrics Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03031155",
   "metadata": {},
   "source": [
    "Loading the classifier that will be used to calculate the *Classifier Score* (CS) and *Fréchet Classifier Distance* (FCD). \\\n",
    "The features of the real data are also precalculated to avoid doing that for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d98fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.models.load_model('../../Classifiers/cifar.h5')\n",
    "feature_layer = classifier.get_layer('features')\n",
    "logits_layer = classifier.get_layer('logits')\n",
    "precalculated_features = utils.fn.calculate_features(classifier, feature_layer, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff56ce9",
   "metadata": {},
   "source": [
    "### 2.3 Hyperparameter Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f10559",
   "metadata": {},
   "source": [
    "These were the hyperparameters tested for the final document. Training all of them simultaneously may take a long time, consider commenting out some options to run the tests individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a5f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "BETA_1 = 0.0\n",
    "BATCH_SIZE = 16\n",
    "LATENT_DIMS = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bb43c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_list = [\n",
    "    {'use_transp_conv':  True, 'smooth_labels': False},\n",
    "    {'use_transp_conv': False, 'smooth_labels': False},\n",
    "    {'use_transp_conv':  True, 'smooth_labels':  True},\n",
    "    {'use_transp_conv': False, 'smooth_labels':  True}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc99c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hparams in hparams_list:\n",
    "    dirname = '{}{}'.format(\n",
    "        'TRPCONV' if hparams['use_transp_conv'] else 'BILINEAR',\n",
    "        '_SMOOTH' if hparams['smooth_labels'] else ''\n",
    "    )\n",
    "    Path(dirname).mkdir(exist_ok=True)\n",
    "    \n",
    "    ## Models\n",
    "    if hparams['use_transp_conv']:\n",
    "        generator = generator_model_transp_conv(LATENT_DIMS)\n",
    "    else:\n",
    "        generator = generator_model_bilinear(LATENT_DIMS)\n",
    "    generator.optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=BETA_1, epsilon=1e-8)\n",
    "\n",
    "    discriminator = discriminator_model()\n",
    "    discriminator.optimizer = tf.keras.optimizers.Adam(learning_rate=2e-4, beta_1=BETA_1, epsilon=1e-8)\n",
    "    discriminator_loss = discriminator_loss_smooth if hparams['smooth_labels'] else discriminator_loss_normal\n",
    "    \n",
    "    ## Callbacks\n",
    "    timer = utils.callback.TimerCallback()\n",
    "    save_samples = utils.callback.SaveSamplesCallback(\n",
    "        path_format=os.path.join(dirname, 'epoch-{}'),\n",
    "        inputs=tf.random.normal((10*10, LATENT_DIMS)),\n",
    "        n_cols=10,\n",
    "        savefig_kwargs={'bbox_inches': 'tight', 'pad_inches': 0, 'dpi': 192},\n",
    "        transform_samples= lambda samples: (1 + samples) * 0.5,\n",
    "        grid_params={'border': 1, 'pad': 1, 'pad_value': 0}\n",
    "    )\n",
    "    metrics = utils.callback.MetricsCallback(\n",
    "        generator=generator,\n",
    "        classifier=classifier,\n",
    "        latent_dims=LATENT_DIMS,\n",
    "        feature_layer=feature_layer,\n",
    "        logits_layer=logits_layer,\n",
    "        n_samples=200*128,\n",
    "        batch_size=128,\n",
    "        precalculated_features=precalculated_features,\n",
    "        save_after=5, save_to=os.path.join(dirname, 'best.h5')\n",
    "    )\n",
    "    \n",
    "    ## Train and save results\n",
    "    train(\n",
    "        generator, discriminator, data, epochs=30,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[timer, save_samples, metrics]\n",
    "    )\n",
    "    \n",
    "    metrics_obj = metrics.get_metrics()\n",
    "    metrics_obj['time'] = timer.get_time()\n",
    "    utils.fn.update_json_log(os.path.join(dirname, 'log.json'), metrics_obj)\n",
    "    \n",
    "    generator.save    (os.path.join(dirname, 'generator.h5'    ), overwrite=True, save_format='h5')\n",
    "    discriminator.save(os.path.join(dirname, 'discriminator.h5'), overwrite=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921fc72",
   "metadata": {},
   "source": [
    "\\\n",
    "In windows the command bellow is used to turn down the machine after the training finishes, very useful if you wanna let the computer running while you go to sleep :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe27611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !shutdown /s /t 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
