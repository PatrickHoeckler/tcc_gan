{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe148eb5",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a92cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_gan as tfgan\n",
    "import numpy as np\n",
    "import os, sys\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append( os.path.abspath('..') )\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c15b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path('Fashion').mkdir(exist_ok=True)\n",
    "os.chdir('Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "data = np.concatenate((x_train, x_test))\n",
    "data = (data.astype('float32') - 127.5) / 127.5  # normalize to [-1, 1]\n",
    "data = np.expand_dims(data, axis=-1)  # add channels dimension\n",
    "assert data.shape == (70000, 28, 28, 1)  # (batch, height, width, channel)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "labels = np.concatenate((y_train, y_test))\n",
    "labels = np.expand_dims(labels, -1)\n",
    "assert labels.shape == (70000, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996911a8",
   "metadata": {},
   "source": [
    "## 1 Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9474049b",
   "metadata": {},
   "source": [
    "### 1.1 Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade13ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_model(latent_dims):\n",
    "    ## Label input\n",
    "    label = tf.keras.Input(shape=(1,), name='label', dtype=tf.int32)\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=36)(label)\n",
    "    label_channel = tf.keras.layers.Dense(7*7)(embedding)\n",
    "    label_channel = tf.keras.layers.Reshape((7, 7, 1))(label_channel)\n",
    "    ## Latent input\n",
    "    seed = tf.keras.Input(shape=(latent_dims,), name='seed')\n",
    "    seed_channels = tf.keras.layers.Dense(7*7*255, input_shape=(latent_dims,))(seed)\n",
    "    seed_channels = tf.keras.layers.Reshape((7, 7, 255))(seed_channels)\n",
    "    \n",
    "    channels = tf.keras.layers.Concatenate(axis=-1)([label_channel, seed_channels])\n",
    "    channels = tf.keras.layers.LeakyReLU()(channels)\n",
    "    channels = tf.keras.layers.BatchNormalization()(channels)\n",
    "    \n",
    "    channels = tf.keras.layers.Conv2D(128, kernel_size=5, strides=1, padding='same')(channels)\n",
    "    channels = tf.keras.layers.LeakyReLU()(channels)\n",
    "    channels = tf.keras.layers.BatchNormalization()(channels)\n",
    "    \n",
    "    channels = tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear')(channels)\n",
    "    channels = tf.keras.layers.Conv2D(64, kernel_size=5, strides=1, padding='same')(channels)\n",
    "    channels = tf.keras.layers.LeakyReLU()(channels)\n",
    "    channels = tf.keras.layers.BatchNormalization()(channels)\n",
    "\n",
    "    channels = tf.keras.layers.UpSampling2D(size=2, interpolation='bilinear')(channels)\n",
    "    img = tf.keras.layers.Conv2D(1, kernel_size=5, strides=1, padding='same', activation='tanh')(channels)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[seed, label], outputs=img, name='generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e945c20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_model():\n",
    "    ## Label input\n",
    "    label = tf.keras.Input(shape=(1,), name='label', dtype=tf.int32)\n",
    "    embedding = tf.keras.layers.Embedding(input_dim=NUM_CLASSES, output_dim=36)(label)\n",
    "    label_channel = tf.keras.layers.Dense(28*28)(embedding)\n",
    "    label_channel = tf.keras.layers.Reshape((28, 28, 1))(label_channel)\n",
    "    \n",
    "    ## Image input\n",
    "    image = tf.keras.Input(shape=(28, 28, 1), name='image')\n",
    "    \n",
    "    channels = tf.keras.layers.Concatenate(axis=-1)([label_channel, image])\n",
    "    \n",
    "    channels = tf.keras.layers.Conv2D(64, kernel_size=5, strides=2, padding='same', input_shape=(28,28,1))(channels)\n",
    "    channels = tf.keras.layers.LeakyReLU()(channels)\n",
    "    channels = tf.keras.layers.Dropout(0.3)(channels)\n",
    "\n",
    "    channels = tf.keras.layers.Conv2D(128, kernel_size=5, strides=2, padding='same')(channels)\n",
    "    channels = tf.keras.layers.LeakyReLU()(channels)\n",
    "    channels = tf.keras.layers.Dropout(0.3)(channels)\n",
    "    \n",
    "    channels = tf.keras.layers.Flatten()(channels)\n",
    "    logit = tf.keras.layers.Dense(1)(channels)\n",
    "    return tf.keras.Model(inputs=[image, label], outputs=logit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e244e66",
   "metadata": {},
   "source": [
    "### 1.2 Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f8296",
   "metadata": {},
   "source": [
    "The binary cross entropy (BCE) between $y$ and $\\hat{y}$ is calculated as:\n",
    "\n",
    "$$\n",
    "    \\mathrm{BCE}(y, \\hat{y}) = - y \\log\\left(\\hat{y}\\right) - (1-y) \\log\\left(1 - \\hat{y}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db60d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb4085",
   "metadata": {},
   "source": [
    "The generator tries to maximize the chance of the discriminator being wrong. This is equivalent of trying to minimize the following loss function:\n",
    "\n",
    "$$\n",
    "    J^{(G)} = -\\log\\bigl(D\\bigl(G(z)\\bigr)\\bigr)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3fcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7e009",
   "metadata": {},
   "source": [
    "The discriminator tries to correctly classify real data as real and fake data as fake. This is equivalent to minimizing the following loss function:\n",
    "\n",
    "$$\n",
    "    J^{(D)} = -\\log\\bigr(D(x)\\bigl) - \\log\\bigl(1 - D\\bigl(G(z)\\bigr)\\bigr)\n",
    "$$\n",
    "\n",
    "Here we scale down the loss by a factor of $\\;0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb44540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_normal(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return 0.5 * (real_loss + fake_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907f873b",
   "metadata": {},
   "source": [
    "This function applies one sided label smoothing of $\\:0.9\\:$ to the discriminator loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305c69c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss_smooth(real_output, fake_output):\n",
    "    real_loss = cross_entropy(0.9 * tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return 0.5 * (real_loss + fake_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d99ddb0",
   "metadata": {},
   "source": [
    "## 2 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab0390d",
   "metadata": {},
   "source": [
    "### 2.1 Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba6e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_train_step(generator, discriminator, images, labels, latent_dims):\n",
    "    noise = tf.random.normal([images.shape[0], latent_dims])\n",
    "    with tf.GradientTape() as disc_tape:\n",
    "        generated_imgs = generator([noise, labels], training=True)\n",
    "        real_output = discriminator([images, labels], training=True)\n",
    "        fake_output = discriminator([generated_imgs, labels], training=True)\n",
    "        loss_D = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "    grads_D = disc_tape.gradient(loss_D, discriminator.trainable_variables)\n",
    "    discriminator.optimizer.apply_gradients(zip(grads_D, discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f936bf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_train_step(generator, discriminator, y, latent_dims):\n",
    "    noise = tf.random.normal([y.shape[0], latent_dims])\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        generated_imgs = generator([noise, y], training=True)\n",
    "        fake_output = discriminator([generated_imgs, y], training=True)\n",
    "        loss_G = generator_loss(fake_output)\n",
    "    \n",
    "    grads_G = gen_tape.gradient(loss_G, generator.trainable_variables)\n",
    "    generator.optimizer.apply_gradients(zip(grads_G, generator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8082374c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(generator, discriminator, data, labels, epochs, batch_size=None, callbacks=None):\n",
    "    latent_dims = generator.input_shape[0][1]\n",
    "    batch_size = batch_size if batch_size is not None else 32\n",
    "    num_batches = 1 + (data.shape[0] - 1) // batch_size\n",
    "    X = tf.data.Dataset.from_tensor_slices(data)\n",
    "    Y = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    dataset = tf.data.Dataset.zip((X, Y)).shuffle(data.shape[0]).batch(batch_size)\n",
    "    \n",
    "    generator_step = tf.function(generator_train_step)\n",
    "    discriminator_step = tf.function(discriminator_train_step)\n",
    "    callbacks = callbacks or []\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        for c in callbacks:\n",
    "            c.on_epoch_begin(epoch=epoch + 1, generator=generator, discriminator=discriminator)\n",
    "            \n",
    "        for images, labels in tqdm(dataset, leave=False, total=num_batches):\n",
    "            discriminator_step(generator, discriminator, images, labels, latent_dims)\n",
    "            generator_step(generator, discriminator, labels, latent_dims)\n",
    "        \n",
    "        for c in callbacks:\n",
    "            c.on_epoch_end(epoch=epoch + 1, generator=generator, discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b1bd2b",
   "metadata": {},
   "source": [
    "### 2.2 Metrics classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac399d4c",
   "metadata": {},
   "source": [
    "Loading the classifier that will be used to calculate the *Classifier Score* (CS) and *Fréchet Classifier Distance* (FCD). \\\n",
    "The features of the real data are also precalculated to avoid doing that for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e386bee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.keras.models.load_model('../../Classifiers/fashion.h5')\n",
    "feature_layer = classifier.get_layer('features')\n",
    "logits_layer = classifier.get_layer('logits')\n",
    "precalculated_features = utils.fn.calculate_features(classifier, feature_layer, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae576cfe",
   "metadata": {},
   "source": [
    "### 2.3 Hyperparameter testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96f0ed",
   "metadata": {},
   "source": [
    "This function will overload the function of the same name in the MetricsCallback instance, this is because the default for this class does not generate the labels as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d91ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_inputs(n_samples):\n",
    "    seeds = tf.random.normal((n_samples, LATENT_DIMS))\n",
    "    labels = tf.random.uniform(\n",
    "        shape=(n_samples, 1),\n",
    "        minval=0, maxval=NUM_CLASSES,\n",
    "        dtype=tf.int32\n",
    "    )\n",
    "    return [seeds, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f10559",
   "metadata": {},
   "source": [
    "These were the hyperparameters tested for the final document. Training all of them simultaneously may take a long time, consider commenting out some options to run the tests individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f8e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENT_DIMS = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ead7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams_list = [\n",
    "    {'batch_size':   16, 'smooth_labels': False},\n",
    "    {'batch_size':   32, 'smooth_labels': False},\n",
    "    {'batch_size':   16, 'smooth_labels': True},\n",
    "    {'batch_size':   32, 'smooth_labels': True},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201b4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hparams in hparams_list:\n",
    "    dirname = 'BS{}{}'.format(\n",
    "        hparams['batch_size'],\n",
    "        '_SMOOTH' if hparams['smooth_labels'] else ''\n",
    "    )\n",
    "    Path(dirname).mkdir(exist_ok=True)\n",
    "    \n",
    "    generator = generator_model(LATENT_DIMS)\n",
    "    generator.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "    discriminator = discriminator_model()\n",
    "    discriminator.optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "    discriminator_loss = discriminator_loss_smooth if hparams['smooth_labels'] else discriminator_loss_normal\n",
    "    \n",
    "    ## Callbacks\n",
    "    timer = utils.callback.TimerCallback()\n",
    "    save_samples = utils.callback.SaveSamplesCallback(\n",
    "        path_format=os.path.join(dirname, 'epoch-{}'),\n",
    "        inputs=[\n",
    "            tf.random.normal((10*10, LATENT_DIMS)),\n",
    "            np.expand_dims(np.repeat(np.arange(10), 10, axis=0), -1)\n",
    "        ],\n",
    "        n_cols=10,\n",
    "        savefig_kwargs={'bbox_inches': 'tight', 'pad_inches': 0, 'dpi': 192},\n",
    "        imshow_kwargs={'cmap': 'gray_r', 'vmin': -1, 'vmax': 1}\n",
    "    )\n",
    "    metrics = utils.callback.MetricsCallback(\n",
    "        generator=generator,\n",
    "        classifier=classifier,\n",
    "        latent_dims=LATENT_DIMS,\n",
    "        feature_layer=feature_layer,\n",
    "        logits_layer=logits_layer,\n",
    "        precalculated_features=precalculated_features,\n",
    "        save_after=5, save_to=os.path.join(dirname, 'best.h5'),\n",
    "    )\n",
    "    metrics.get_random_inputs = get_random_inputs #overloading default function\n",
    "    \n",
    "    ## Train and save results\n",
    "    train(\n",
    "        generator, discriminator, data, labels, epochs=30,\n",
    "        batch_size=hparams['batch_size'],\n",
    "        callbacks=[timer, save_samples, metrics]\n",
    "    )\n",
    "    \n",
    "    metrics_obj = metrics.get_metrics()\n",
    "    metrics_obj['time'] = timer.get_time()\n",
    "    utils.fn.update_json_log(os.path.join(dirname, 'log.json'), metrics_obj)\n",
    "    \n",
    "    generator.save    (os.path.join(dirname, 'generator.h5'    ), overwrite=True, save_format='h5')\n",
    "    discriminator.save(os.path.join(dirname, 'discriminator.h5'), overwrite=True, save_format='h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c72b9",
   "metadata": {},
   "source": [
    "\\\n",
    "In windows the command bellow is used to turn down the machine after the training finishes, very useful if you wanna let the computer running while you go to sleep :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c85ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !shutdown /s /t 60"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
